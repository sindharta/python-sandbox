{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4df3801-545c-4ecb-9acf-8239aa169526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intents': [{'tag': 'greeting', 'patterns': ['Hello', 'Hi', 'I need help', 'Hey'], 'responses': ['Hi there! How can I help?', 'Hello, and welcome to this chatbot'], 'context_set': ''}, {'tag': 'bye', 'patterns': ['Thank you for the help', 'Bye', 'Great thanks'], 'responses': ['Do you have any further questions?', 'Thanks for asking a question']}, {'tag': 'courses', 'patterns': ['What are the courses available?', 'Do you have courses?'], 'responses': ['We have courses on creative design, programming and machine learning', 'We have over 300 courses available']}, {'tag': 'coding', 'patterns': ['What coding courses do you have?', 'I want to learn programming'], 'responses': ['We have many courses, including Hello Coding and Python for Automation', 'Check out our site listing for a complete list of courses']}, {'tag': 'machinelearning', 'patterns': ['What machine learning courses do you teach?', 'Do you teach AI?', 'I want to learn artificial intelligence'], 'responses': ['We have Complete Machine Learning, ChatGPT Bundle and much more', 'Find top machine learning courses in our site catalogue']}, {'tag': 'creative', 'patterns': ['Do you teach creative courses', 'Do you have non coding courses', ' I want to learn something else'], 'responses': ['We have tons of non coding courses', 'Check our site for more courses', ' We teach video editing, audio production, graphic design and much more!']}]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "with open(\"data.json\") as json_data:\n",
    "    data = json.load(json_data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fcd9457-6d73-46b1-9dd2-8800b2746b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9293950e-455b-476f-b9e5-28f5e186b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "pattern_words_with_tag = []\n",
    "classes = []\n",
    "\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        pattern_words = nltk.word_tokenize(pattern)\n",
    "\n",
    "        words.extend(pattern_words)\n",
    "        pattern_words_with_tag.append((pattern_words, intent[\"tag\"])) # Tuple type\n",
    "\n",
    "        if intent[\"tag\"] not in classes:\n",
    "            classes.append(intent[\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c54fe27-7caa-41e4-9fca-5ca641fe0066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Hi', 'I', 'need', 'help', 'Hey', 'Thank', 'you', 'for', 'the', 'help', 'Bye', 'Great', 'thanks', 'What', 'are', 'the', 'courses', 'available', '?', 'Do', 'you', 'have', 'courses', '?', 'What', 'coding', 'courses', 'do', 'you', 'have', '?', 'I', 'want', 'to', 'learn', 'programming', 'What', 'machine', 'learning', 'courses', 'do', 'you', 'teach', '?', 'Do', 'you', 'teach', 'AI', '?', 'I', 'want', 'to', 'learn', 'artificial', 'intelligence', 'Do', 'you', 'teach', 'creative', 'courses', 'Do', 'you', 'have', 'non', 'coding', 'courses', 'I', 'want', 'to', 'learn', 'something', 'else']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "513de0a2-52b2-4d02-9dc4-3370d8c5bcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['Hello'], 'greeting'), (['Hi'], 'greeting'), (['I', 'need', 'help'], 'greeting'), (['Hey'], 'greeting'), (['Thank', 'you', 'for', 'the', 'help'], 'bye'), (['Bye'], 'bye'), (['Great', 'thanks'], 'bye'), (['What', 'are', 'the', 'courses', 'available', '?'], 'courses'), (['Do', 'you', 'have', 'courses', '?'], 'courses'), (['What', 'coding', 'courses', 'do', 'you', 'have', '?'], 'coding'), (['I', 'want', 'to', 'learn', 'programming'], 'coding'), (['What', 'machine', 'learning', 'courses', 'do', 'you', 'teach', '?'], 'machinelearning'), (['Do', 'you', 'teach', 'AI', '?'], 'machinelearning'), (['I', 'want', 'to', 'learn', 'artificial', 'intelligence'], 'machinelearning'), (['Do', 'you', 'teach', 'creative', 'courses'], 'creative'), (['Do', 'you', 'have', 'non', 'coding', 'courses'], 'creative'), (['I', 'want', 'to', 'learn', 'something', 'else'], 'creative')]\n"
     ]
    }
   ],
   "source": [
    "#a list of tuples\n",
    "print(pattern_words_with_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aacbaaf0-8381-4330-aa98-fdb2ef0ca587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['greeting', 'bye', 'courses', 'coding', 'machinelearning', 'creative']\n"
     ]
    }
   ],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68a3dbec-8952-4468-84a3-b823e101f13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['?', 'ai', 'ar', 'art', 'avail', 'bye', 'cod', 'cours', 'cre', 'do', 'els', 'for', 'gre', 'hav', 'hello', 'help', 'hey', 'hi', 'i', 'intellig', 'learn', 'machin', 'nee', 'non', 'program', 'someth', 'teach', 'thank', 'the', 'to', 'want', 'what', 'you']\n"
     ]
    }
   ],
   "source": [
    "#  Clean chat data for machine learning\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "words_lowercase = [stemmer.stem(word.lower()) for word in words]\n",
    "\n",
    "# converting to set will remove duplicate elements\n",
    "unique_words = sorted(list(set(words_lowercase)))\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c803008-bdbd-4d6f-abd3-b687de0589ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "empty_output = [0] * len(classes)\n",
    "print(empty_output)\n",
    "\n",
    "output_row = list(empty_output)\n",
    "print(output_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06f9a36b-d5b5-4470-9a3d-30bfb755cd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]], [[1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0]], [[1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0]], [[1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 1, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0], [0, 0, 0, 1, 0, 0]], [[1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 0]], [[1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0]], [[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 1, 0]], [[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]], [[0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 0, 1]]]\n"
     ]
    }
   ],
   "source": [
    "#  Build bag of words for ML model\n",
    "\n",
    "# print(documents)\n",
    "empty_output = [0] * len(classes)\n",
    "# print(empty_output)\n",
    "\n",
    "training_data = []\n",
    "\n",
    "for tuple in pattern_words_with_tag:\n",
    "    bag_of_words = []\n",
    "\n",
    "    # Tuple: ([pattern_words], tag)\n",
    "    pattern_words = tuple[0]\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "\n",
    "    for unique_word in unique_words:\n",
    "        bag_of_words.append(1) if unique_word in pattern_words else bag_of_words.append(0)\n",
    "\n",
    "    output_row = list(empty_output)\n",
    "    output_row[classes.index(tuple[1])] = 1\n",
    "    training_data.append([bag_of_words, output_row])\n",
    "\n",
    "#print(pattern_words)\n",
    "\n",
    "#training_data: a list of a tuple of ([bag of words], [bag of tag])\n",
    "#https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "print(training_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09154986-690c-4b06-a709-d478efebd5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d4f9072-bca1-4cc8-8d87-4fe2cf138e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1], [1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]]\n",
      "[[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0], [0, 0, 1, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "#  Split data for machine learning\n",
    "\n",
    "import random\n",
    "random.shuffle(training_data)\n",
    "\n",
    "#print(training_data)\n",
    "#print(type(training_data))\n",
    "\n",
    "import numpy\n",
    "training_numpy = numpy.array(training_data, dtype=object)\n",
    "\n",
    "#print(training_numpy)\n",
    "#print(type(training_numpy))\n",
    "\n",
    "train_X = list(training_numpy[:,0]) # to access column 0\n",
    "\n",
    "print(train_X)\n",
    "#print(len(train_X))\n",
    "\n",
    "train_Y = list(training_numpy[:,1]) # to access column 1\n",
    "print(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f43a41ec-a653-4859-a5dd-4bc299949d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 5999  | total loss: \u001b[1m\u001b[32m0.00046\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 2000 | loss: 0.00046 - acc: 1.0000 -- iter: 16/17\n",
      "Training Step: 6000  | total loss: \u001b[1m\u001b[32m0.00047\u001b[0m\u001b[0m | time: 0.016s\n",
      "| Adam | epoch: 2000 | loss: 0.00047 - acc: 1.0000 -- iter: 17/17\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "#  Build a TensorFlow machine learning model for chat\n",
    "\n",
    "import tflearn \n",
    "neural_network = tflearn.input_data(shape = [None, len(train_X[0])])\n",
    "print(neural_network)\n",
    "\n",
    "neural_network = tflearn.fully_connected(neural_network, 8)\n",
    "print(neural_network)\n",
    "\n",
    "neural_network = tflearn.fully_connected(neural_network, 8)\n",
    "print(neural_network)\n",
    "\n",
    "neural_network = tflearn.fully_connected(neural_network, len(train_Y[0]), activation=\"softmax\")\n",
    "print(neural_network)\n",
    "\n",
    "neural_network = tflearn.regression(neural_network)\n",
    "print(neural_network)\n",
    "\n",
    "model = tflearn.DNN(neural_network)\n",
    "print(model)\n",
    "\n",
    "model.fit(train_X, train_Y, n_epoch = 2000, batch_size = 8, show_metric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7ef22d1-ea57-4af7-a7e3-fad4fbf4f0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:C:\\Users\\sin\\G\\Personal\\python-sandbox\\ChatBot\\chatbot_dnn.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# Test chatbot machine learning model\n",
    "model.save(\"chatbot_dnn.tflearn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e858f93c-1661-42a8-abe9-772b9da68ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Hi', 'I', 'need', 'help', 'Hey', 'Thank', 'you', 'for', 'the', 'help', 'Bye', 'Great', 'thanks', 'What', 'are', 'the', 'courses', 'available', '?', 'Do', 'you', 'have', 'courses', '?', 'What', 'coding', 'courses', 'do', 'you', 'have', '?', 'I', 'want', 'to', 'learn', 'programming', 'What', 'machine', 'learning', 'courses', 'do', 'you', 'teach', '?', 'Do', 'you', 'teach', 'AI', '?', 'I', 'want', 'to', 'learn', 'artificial', 'intelligence', 'Do', 'you', 'teach', 'creative', 'courses', 'Do', 'you', 'have', 'non', 'coding', 'courses', 'I', 'want', 'to', 'learn', 'something', 'else']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ad54c91-8f71-49de-82f9-292afae92f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\sin\\G\\Personal\\python-sandbox\\ChatBot\\chatbot_dnn.tflearn\n",
      "<tflearn.models.dnn.DNN object at 0x00000137CB883CA0>\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "model.load(\"chatbot_dnn.tflearn\")\n",
    "\n",
    "print(model)\n",
    "\n",
    "question = \"Do you sell any coding course?\"\n",
    "\n",
    "def process_question(question):\n",
    "    question_tokenized = nltk.word_tokenize(question)\n",
    "    question_stemmed = [stemmer.stem(word.lower()) for word in question_tokenized]\n",
    "\n",
    "    bag = [0] * len(unique_words)\n",
    "\n",
    "    for stem in question_stemmed:\n",
    "        for index, word in enumerate(unique_words):\n",
    "            if word == stem:\n",
    "                bag[index] = 1\n",
    "\n",
    "    return(numpy.array(bag))\n",
    "\n",
    "processed_question = process_question(question)\n",
    "print(len(processed_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9dc6394-1e4a-4457-884a-57e7fa29767c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.6559168e-09 8.0910176e-08 3.6639106e-04 8.4420657e-01 3.4384280e-05\n",
      "  1.5539253e-01]]\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict([processed_question])\n",
    "print(prediction)\n",
    "#print(classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
